Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.0
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2359461784362793
	train_negative_loss: 2.280427932739258
	train_positive_acc: 0.6199914659362796
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.06799715757369995
	train_incorrect_loss: nan
	train_positive_loss: 0.06544260680675507
	train_negative_loss: 0.06995612382888794
	train_correct_acc: 0.9779024390243903
	train_incorrect_acc: nan
	train_positive_acc: 0.9877810219919043
	train_negative_acc: 0.9678920180226849
	train_correct_nonzero: 41000
	train_incorrect_nonzero: 0
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.013669086620211601
	val_negative_loss: 0.000507329881656915
	val_positive_acc: 1.0
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.026514917612075806
	test_negative_loss: 0.0068749235942959785
	test_positive_acc: 0.9903375670748715
	test_negative_acc: 0.997187815975733
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 8.559590816497803 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.1
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.235369920730591
	train_negative_loss: 2.2804365158081055
	train_positive_acc: 0.6440865262563049
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.11839313060045242
	train_incorrect_loss: 2.2429363119807233
	train_positive_loss: 0.2551599144935608
	train_negative_loss: 0.18259559571743011
	train_correct_acc: 0.9739423996247981
	train_incorrect_acc: 0.04282807010079737
	train_positive_acc: 0.9901726675350319
	train_negative_acc: 0.8660124665275255
	train_correct_nonzero: 39000
	train_incorrect_nonzero: 2000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.013614103198051453
	val_negative_loss: 0.06034106761217117
	val_positive_acc: 1.0
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.02879374288022518
	test_negative_loss: 0.06322630494832993
	test_positive_acc: 0.9844454087530342
	test_negative_acc: 0.9960085706927142
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.370265007019043 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.2
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2349014282226562
	train_negative_loss: 2.2804605960845947
	train_positive_acc: 0.6636901567956638
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.16960982978343964
	train_incorrect_loss: 1.5650131702423096
	train_positive_loss: 0.3050680458545685
	train_negative_loss: 0.3071083724498749
	train_correct_acc: 0.9661905985749011
	train_incorrect_acc: 0.0640525390041172
	train_positive_acc: 0.9916411688153439
	train_negative_acc: 0.7646283135599676
	train_correct_nonzero: 37000
	train_incorrect_nonzero: 4000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.013439379632472992
	val_negative_loss: 0.15564262866973877
	val_positive_acc: 1.0
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.026793142780661583
	test_negative_loss: 0.15678276121616364
	test_positive_acc: 0.9868576153925644
	test_negative_acc: 0.9948632599346316
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.454211473464966 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.3
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2344675064086914
	train_negative_loss: 2.2803616523742676
	train_positive_acc: 0.6782248793445469
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.21608245372772217
	train_incorrect_loss: 1.1696209907531738
	train_positive_loss: 0.30809637904167175
	train_negative_loss: 0.4422185719013214
	train_correct_acc: 0.9552342623196324
	train_incorrect_acc: 0.1056338117900428
	train_positive_acc: 0.9931544456735043
	train_negative_acc: 0.6692965055238025
	train_correct_nonzero: 35000
	train_incorrect_nonzero: 6000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.021751422435045242
	val_negative_loss: 0.3026300370693207
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.038440536707639694
	test_negative_loss: 0.2979884147644043
	test_positive_acc: 0.9816804024360962
	test_negative_acc: 0.994711854895872
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.249090909957886 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.4
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2340736389160156
	train_negative_loss: 2.2804293632507324
	train_positive_acc: 0.6916150147276573
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.25888732075691223
	train_incorrect_loss: 0.8980068564414978
	train_positive_loss: 0.28901025652885437
	train_negative_loss: 0.5994352698326111
	train_correct_acc: 0.9306457241895044
	train_incorrect_acc: 0.1880690074158312
	train_positive_acc: 0.9957307152842315
	train_negative_acc: 0.575808713896637
	train_correct_nonzero: 33000
	train_incorrect_nonzero: 8000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.011313783004879951
	val_negative_loss: 0.46998655796051025
	val_positive_acc: 1.0
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.022636599838733673
	test_negative_loss: 0.46342915296554565
	test_positive_acc: 0.9893893257035301
	test_negative_acc: 0.9846382146459602
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.213952541351318 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.425
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.234005928039551
	train_negative_loss: 2.2804055213928223
	train_positive_acc: 0.6950520059567857
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2685556411743164
	train_incorrect_loss: 0.8410231471061707
	train_positive_loss: 0.2817554771900177
	train_negative_loss: 0.6408877372741699
	train_correct_acc: 0.9129028813055755
	train_incorrect_acc: 0.24770964521860114
	train_positive_acc: 0.9964698185914641
	train_negative_acc: 0.5543464478214297
	train_correct_nonzero: 32500
	train_incorrect_nonzero: 8500
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.008384382352232933
	val_negative_loss: 0.5545368790626526
	val_positive_acc: 1.0
	val_negative_acc: 0.9407763105242097
test:
	test_positive_loss: 0.01347757875919342
	test_negative_loss: 0.5537163019180298
	test_positive_acc: 0.9988839285714286
	test_negative_acc: 0.9364295966458359
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.252195596694946 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.45
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2339141368865967
	train_negative_loss: 2.2803702354431152
	train_positive_acc: 0.6986231899976517
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2793882191181183
	train_incorrect_loss: 0.7838723063468933
	train_positive_loss: 0.27353087067604065
	train_negative_loss: 0.6900078058242798
	train_correct_acc: 0.8913011614877694
	train_incorrect_acc: 0.3236550555543749
	train_positive_acc: 0.9966039949288649
	train_negative_acc: 0.5380809269430092
	train_correct_nonzero: 32000
	train_incorrect_nonzero: 9000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.006223221775144339
	val_negative_loss: 0.6354674100875854
	val_positive_acc: 1.0
	val_negative_acc: 0.8003201280512204
test:
	test_positive_loss: 0.009243655018508434
	test_negative_loss: 0.6236523389816284
	test_positive_acc: 1.0
	test_negative_acc: 0.8101176018531341
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.23818564414978 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.475
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2338743209838867
	train_negative_loss: 2.2802882194519043
	train_positive_acc: 0.7011720198306811
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.28826430439949036
	train_incorrect_loss: 0.7333188056945801
	train_positive_loss: 0.2653229832649231
	train_negative_loss: 0.735433042049408
	train_correct_acc: 0.8693448586627608
	train_incorrect_acc: 0.3943130064827071
	train_positive_acc: 0.9973519870670384
	train_negative_acc: 0.52204270230766
	train_correct_nonzero: 31500
	train_incorrect_nonzero: 9500
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.00729786604642868
	val_negative_loss: 0.6281419992446899
	val_positive_acc: 1.0
	val_negative_acc: 0.8509403761504601
test:
	test_positive_loss: 0.010632725432515144
	test_negative_loss: 0.612288773059845
	test_positive_acc: 0.9986702127659575
	test_negative_acc: 0.8597754327852288
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.230150938034058 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.5
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233815908432007
	train_negative_loss: 2.2802610397338867
	train_positive_acc: 0.703469204574813
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.30020466446876526
	train_incorrect_loss: 0.6826578974723816
	train_positive_loss: 0.2567487061023712
	train_negative_loss: 0.7900698781013489
	train_correct_acc: 0.8363905381802998
	train_incorrect_acc: 0.5083410366743735
	train_positive_acc: 0.9978564070318545
	train_negative_acc: 0.5150106849185064
	train_correct_nonzero: 31000
	train_incorrect_nonzero: 10000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.006345580331981182
	val_negative_loss: 0.6624513864517212
	val_positive_acc: 1.0
	val_negative_acc: 0.8105242096838736
test:
	test_positive_loss: 0.008837158791720867
	test_negative_loss: 0.654213547706604
	test_positive_acc: 1.0
	test_negative_acc: 0.785170601579913
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.203711748123169 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.525
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2337334156036377
	train_negative_loss: 2.2802443504333496
	train_positive_acc: 0.706174908181372
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.3084767758846283
	train_incorrect_loss: 0.6348721385002136
	train_positive_loss: 0.24702274799346924
	train_negative_loss: 0.8419679999351501
	train_correct_acc: 0.8047413801163297
	train_incorrect_acc: 0.6209521302398867
	train_positive_acc: 0.998133189722911
	train_negative_acc: 0.5159717895777647
	train_correct_nonzero: 30500
	train_incorrect_nonzero: 10500
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.011106610298156738
	val_negative_loss: 0.6985743045806885
	val_positive_acc: 1.0
	val_negative_acc: 0.6304521808723489
test:
	test_positive_loss: 0.01473759114742279
	test_negative_loss: 0.6914076209068298
	test_positive_acc: 1.0
	test_negative_acc: 0.6070035583171207
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.202507734298706 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.55
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2336223125457764
	train_negative_loss: 2.280195474624634
	train_positive_acc: 0.7090817095567222
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.31709951162338257
	train_incorrect_loss: 0.5906685590744019
	train_positive_loss: 0.2372821569442749
	train_negative_loss: 0.8986563086509705
	train_correct_acc: 0.7792185247848867
	train_incorrect_acc: 0.7247799006708822
	train_positive_acc: 0.9983214335348481
	train_negative_acc: 0.5304842801759232
	train_correct_nonzero: 30000
	train_incorrect_nonzero: 11000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.007828110828995705
	val_negative_loss: 0.73395836353302
	val_positive_acc: 1.0
	val_negative_acc: 0.48059223689475794
test:
	test_positive_loss: 0.011301584541797638
	test_negative_loss: 0.724422812461853
	test_positive_acc: 1.0
	test_negative_acc: 0.4845135937998021
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.259338617324829 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.575
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2335517406463623
	train_negative_loss: 2.2802343368530273
	train_positive_acc: 0.7119311531744902
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.3255130648612976
	train_incorrect_loss: 0.5498131513595581
	train_positive_loss: 0.22756540775299072
	train_negative_loss: 0.9601001143455505
	train_correct_acc: 0.7618829103949372
	train_incorrect_acc: 0.8047391756236196
	train_positive_acc: 0.9982716575268841
	train_negative_acc: 0.5491426980306465
	train_correct_nonzero: 29500
	train_incorrect_nonzero: 11500
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.0062160673551261425
	val_negative_loss: 0.8489698171615601
	val_positive_acc: 1.0
	val_negative_acc: 0.050220088035214085
test:
	test_positive_loss: 0.009206576272845268
	test_negative_loss: 0.8393807411193848
	test_positive_acc: 1.0
	test_negative_acc: 0.10288215312668969
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.204183101654053 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.6
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2334885597229004
	train_negative_loss: 2.280179023742676
	train_positive_acc: 0.7146894972368937
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.3326077461242676
	train_incorrect_loss: 0.5077832341194153
	train_positive_loss: 0.21696490049362183
	train_negative_loss: 1.0243535041809082
	train_correct_acc: 0.7484501723481487
	train_incorrect_acc: 0.8787117240304424
	train_positive_acc: 0.9983722465429782
	train_negative_acc: 0.573919218152781
	train_correct_nonzero: 29000
	train_incorrect_nonzero: 12000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.00948425941169262
	val_negative_loss: 0.8035136461257935
	val_positive_acc: 1.0
	val_negative_acc: 0.09963985594237695
test:
	test_positive_loss: 0.013811999931931496
	test_negative_loss: 0.7966611385345459
	test_positive_acc: 1.0
	test_negative_acc: 0.1751765420835809
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.168473482131958 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.7
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233248472213745
	train_negative_loss: 2.280139207839966
	train_positive_acc: 0.7253573937643616
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.3465842008590698
	train_incorrect_loss: 0.3616102635860443
	train_positive_loss: 0.17187628149986267
	train_negative_loss: 1.3116267919540405
	train_correct_acc: 0.7611669415096131
	train_incorrect_acc: 0.9915699922550109
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.6817928798715969
	train_correct_nonzero: 27000
	train_incorrect_nonzero: 14000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.011493481695652008
	val_negative_loss: 1.0399582386016846
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.016132088378071785
	test_negative_loss: 1.0230872631072998
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.185415744781494 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.8
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233032703399658
	train_negative_loss: 2.2798879146575928
	train_positive_acc: 0.7337170572829933
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.3405456244945526
	train_incorrect_loss: 0.23821552097797394
	train_positive_loss: 0.12540902197360992
	train_negative_loss: 1.730136513710022
	train_correct_acc: 0.8182245887706139
	train_incorrect_acc: 0.9990585332048746
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.7799796662302672
	train_correct_nonzero: 25000
	train_incorrect_nonzero: 16000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.0047085098922252655
	val_negative_loss: 1.389188528060913
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.0071776071563363075
	test_negative_loss: 1.391577959060669
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.231905937194824 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.9
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2328503131866455
	train_negative_loss: 2.279771566390991
	train_positive_acc: 0.7412040014915415
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2909996509552002
	train_incorrect_loss: 0.12878331542015076
	train_positive_loss: 0.07740384340286255
	train_negative_loss: 2.4278399355545606
	train_correct_acc: 0.8894002258113436
	train_incorrect_acc: 0.9990243902439025
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.8772094904306628
	train_correct_nonzero: 23000
	train_incorrect_nonzero: 18000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.005753508768975735
	val_negative_loss: 1.8298002481460571
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.006856079678982496
	test_negative_loss: 1.812825322151184
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.235346794128418 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.925
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232835531234741
	train_negative_loss: 2.2798756678116145
	train_positive_acc: 0.7423495478302093
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2716047465801239
	train_incorrect_loss: 0.10131821036338806
	train_positive_loss: 0.06517936289310455
	train_negative_loss: 2.73034924415532
	train_correct_acc: 0.9092497534838239
	train_incorrect_acc: 0.9990243902439025
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.9015195507657353
	train_correct_nonzero: 22500
	train_incorrect_nonzero: 18500
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.007766443304717541
	val_negative_loss: 2.0943074226379395
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.008808012120425701
	test_negative_loss: 2.082228899002075
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.220709562301636 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.95
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232799768447876
	train_negative_loss: 2.2797041489527774
	train_positive_acc: 0.7442354780786044
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2355569303035736
	train_incorrect_loss: 0.07786703109741211
	train_positive_loss: 0.05297544598579407
	train_negative_loss: 3.037227053092536
	train_correct_acc: 0.9299307450357178
	train_incorrect_acc: 0.9990243902439025
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.9260134090870549
	train_correct_nonzero: 22000
	train_incorrect_nonzero: 19000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.005696265958249569
	val_negative_loss: 2.3733010292053223
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.0064418725669384
	test_negative_loss: 2.359313488006592
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.268794536590576 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.975
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232758045196533
	train_negative_loss: 2.279218815286239
	train_positive_acc: 0.7458887273052552
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1905222237110138
	train_incorrect_loss: 0.05625324696302414
	train_positive_loss: 0.041148971766233444
	train_negative_loss: 3.554051942192018
	train_correct_acc: 0.9516490775161165
	train_incorrect_acc: 0.9989925768822906
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.9503733617681754
	train_correct_nonzero: 21500
	train_incorrect_nonzero: 19500
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.003504999214783311
	val_negative_loss: 3.0657272338867188
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.003945057280361652
	test_negative_loss: 3.029362201690674
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.227083444595337 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: naive
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 1.0
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232710838317871
	train_negative_loss: 2.2789623569322113
	train_positive_acc: 0.7477984962230337
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1293589174747467
	train_incorrect_loss: 0.03516846522688866
	train_positive_loss: 0.029383914545178413
	train_negative_loss: 4.48071221129535
	train_correct_acc: 0.9744505360840583
	train_incorrect_acc: 0.9989925768822906
	train_positive_acc: 0.9984191508581752
	train_negative_acc: 0.9747791251721514
	train_correct_nonzero: 21000
	train_incorrect_nonzero: 20000
	train_positive_nonzero: 20500
	train_negative_nonzero: 20500
val:
	val_positive_loss: 0.004188706632703543
	val_negative_loss: 3.3322882652282715
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.004497433081269264
	test_negative_loss: 3.354401111602783
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 7.238638877868652 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.0
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2359461784362793
	train_negative_loss: 2.280427932739258
	train_positive_acc: 0.6199914659362796
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.09650103747844696
	train_incorrect_loss: nan
	train_positive_loss: 0.09553728997707367
	train_negative_loss: 0.09688679873943329
	train_correct_acc: 0.9747073170731708
	train_incorrect_acc: nan
	train_positive_acc: 0.9799779072401986
	train_negative_acc: 0.9696778654886106
	train_correct_nonzero: 24468
	train_incorrect_nonzero: 0
	train_positive_nonzero: 10199
	train_negative_nonzero: 14269
val:
	val_positive_loss: 0.0025768796913325787
	val_negative_loss: 0.018073633313179016
	val_positive_acc: 1.0
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.004708269611001015
	test_negative_loss: 0.04248826950788498
	test_positive_acc: 1.0
	test_negative_acc: 0.9854852021416316
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.811348676681519 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.1
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.235369920730591
	train_negative_loss: 2.2804365158081055
	train_positive_acc: 0.6440865262563049
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.13170485198497772
	train_incorrect_loss: 2.984931470135213
	train_positive_loss: 0.3691520094871521
	train_negative_loss: 0.14604859054088593
	train_correct_acc: 0.9693254139785611
	train_incorrect_acc: 0.03485530985530986
	train_positive_acc: 0.974241479462403
	train_negative_acc: 0.8739738012678285
	train_correct_nonzero: 23014
	train_incorrect_nonzero: 313
	train_positive_nonzero: 6963
	train_negative_nonzero: 16364
val:
	val_positive_loss: 0.07467061281204224
	val_negative_loss: 0.016279367730021477
	val_positive_acc: 0.9501800720288115
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.07213956862688065
	test_negative_loss: 0.018865302205085754
	test_positive_acc: 0.9699476761894394
	test_negative_acc: 1.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.837674379348755 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.2
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2349014282226562
	train_negative_loss: 2.2804605960845947
	train_positive_acc: 0.6636901567956638
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.14903652667999268
	train_incorrect_loss: 2.7863481044769287
	train_positive_loss: 0.5606563687324524
	train_negative_loss: 0.1706649363040924
	train_correct_acc: 0.9597508779891419
	train_incorrect_acc: 0.051523591584567194
	train_positive_acc: 0.9684086427015173
	train_negative_acc: 0.7755413521398586
	train_correct_nonzero: 21184
	train_incorrect_nonzero: 538
	train_positive_nonzero: 6656
	train_negative_nonzero: 15066
val:
	val_positive_loss: 0.02131696604192257
	val_negative_loss: 0.01752336695790291
	val_positive_acc: 1.0
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.04604567959904671
	test_negative_loss: 0.021847154945135117
	test_positive_acc: 0.9820239295230444
	test_negative_acc: 0.996165343267965
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.829777956008911 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.3
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2344675064086914
	train_negative_loss: 2.2803616523742676
	train_positive_acc: 0.6782248793445469
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.16695506870746613
	train_incorrect_loss: 2.4625325202941895
	train_positive_loss: 0.6633373498916626
	train_negative_loss: 0.2004653811454773
	train_correct_acc: 0.9584385784114534
	train_incorrect_acc: 0.0515355710497664
	train_positive_acc: 0.9653357371078048
	train_negative_acc: 0.6868575457355489
	train_correct_nonzero: 18699
	train_incorrect_nonzero: 799
	train_positive_nonzero: 5852
	train_negative_nonzero: 13646
val:
	val_positive_loss: 0.067604199051857
	val_negative_loss: 0.025749608874320984
	val_positive_acc: 0.970188075230092
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.06825773417949677
	test_negative_loss: 0.025481145828962326
	test_positive_acc: 0.9691300106471976
	test_negative_acc: 0.998546511627907
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.97113823890686 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.4
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2340736389160156
	train_negative_loss: 2.2804293632507324
	train_positive_acc: 0.6916150147276573
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1863616555929184
	train_incorrect_loss: 2.3933119773864746
	train_positive_loss: 0.7857953310012817
	train_negative_loss: 0.22331923246383667
	train_correct_acc: 0.9469843394244081
	train_incorrect_acc: 0.06561543635198903
	train_positive_acc: 0.9543501353688373
	train_negative_acc: 0.5973668066325297
	train_correct_nonzero: 17624
	train_incorrect_nonzero: 890
	train_positive_nonzero: 6315
	train_negative_nonzero: 12199
val:
	val_positive_loss: 0.024904776364564896
	val_negative_loss: 0.03872830793261528
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.02747364528477192
	test_negative_loss: 0.054047174751758575
	test_positive_acc: 0.9883902263001754
	test_negative_acc: 0.9826678715429247
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.927164554595947 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.425
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.234005928039551
	train_negative_loss: 2.2804055213928223
	train_positive_acc: 0.6950520059567857
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1848618984222412
	train_incorrect_loss: 2.3809351921081543
	train_positive_loss: 0.8072525858879089
	train_negative_loss: 0.22752755880355835
	train_correct_acc: 0.9519092642478164
	train_incorrect_acc: 0.07100776607883699
	train_positive_acc: 0.9646054206774457
	train_negative_acc: 0.5754264202818236
	train_correct_nonzero: 16492
	train_incorrect_nonzero: 920
	train_positive_nonzero: 5658
	train_negative_nonzero: 11754
val:
	val_positive_loss: 0.022690588608384132
	val_negative_loss: 0.1488109529018402
	val_positive_acc: 1.0
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.026148993521928787
	test_negative_loss: 0.153285950422287
	test_positive_acc: 0.9963967339299786
	test_negative_acc: 0.9855418144604722
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.935249090194702 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.45
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2339141368865967
	train_negative_loss: 2.2803702354431152
	train_positive_acc: 0.6986231899976517
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.20166386663913727
	train_incorrect_loss: 2.315563201904297
	train_positive_loss: 0.8176344037055969
	train_negative_loss: 0.25503990054130554
	train_correct_acc: 0.9346554632825808
	train_incorrect_acc: 0.09803123272405555
	train_positive_acc: 0.9518521628516367
	train_negative_acc: 0.5530872279512488
	train_correct_nonzero: 16069
	train_incorrect_nonzero: 948
	train_positive_nonzero: 5620
	train_negative_nonzero: 11397
val:
	val_positive_loss: 0.027344781905412674
	val_negative_loss: 0.05124583840370178
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.027437549084424973
	test_negative_loss: 0.06510089337825775
	test_positive_acc: 0.9920348283490328
	test_negative_acc: 0.9855418144604722
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.789875745773315 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.475
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2338743209838867
	train_negative_loss: 2.2802882194519043
	train_positive_acc: 0.7011720198306811
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.20654062926769257
	train_incorrect_loss: 2.3154306411743164
	train_positive_loss: 0.8481457233428955
	train_negative_loss: 0.2634449303150177
	train_correct_acc: 0.9389212213505915
	train_incorrect_acc: 0.09449555209149324
	train_positive_acc: 0.9566435177289649
	train_negative_acc: 0.532815109452598
	train_correct_nonzero: 15961
	train_incorrect_nonzero: 989
	train_positive_nonzero: 5986
	train_negative_nonzero: 10964
val:
	val_positive_loss: 0.04865033179521561
	val_negative_loss: 0.03765875846147537
	val_positive_acc: 0.9799919967987194
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.040451932698488235
	test_negative_loss: 0.047913119196891785
	test_positive_acc: 0.9822680891754487
	test_negative_acc: 0.9891215178125539
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.875746488571167 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.5
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233815908432007
	train_negative_loss: 2.2802610397338867
	train_positive_acc: 0.703469204574813
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.20837447047233582
	train_incorrect_loss: 2.1732981204986572
	train_positive_loss: 0.8248661160469055
	train_negative_loss: 0.26790863275527954
	train_correct_acc: 0.9372119247359357
	train_incorrect_acc: 0.09550405724013318
	train_positive_acc: 0.9559286353474186
	train_negative_acc: 0.508874849611397
	train_correct_nonzero: 15490
	train_incorrect_nonzero: 1043
	train_positive_nonzero: 5855
	train_negative_nonzero: 10678
val:
	val_positive_loss: 0.0371110662817955
	val_negative_loss: 0.01809469610452652
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.04610679671168327
	test_negative_loss: 0.028527062386274338
	test_positive_acc: 0.9794612268103803
	test_negative_acc: 0.9911529400726232
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.98116660118103 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.525
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2337334156036377
	train_negative_loss: 2.2802443504333496
	train_positive_acc: 0.706174908181372
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2083609402179718
	train_incorrect_loss: 2.191910982131958
	train_positive_loss: 0.851524829864502
	train_negative_loss: 0.2750324010848999
	train_correct_acc: 0.9363527585666921
	train_incorrect_acc: 0.10360590325084419
	train_positive_acc: 0.9559925493580389
	train_negative_acc: 0.4934682850447578
	train_correct_nonzero: 14715
	train_incorrect_nonzero: 976
	train_positive_nonzero: 5548
	train_negative_nonzero: 10143
val:
	val_positive_loss: 0.055850788950920105
	val_negative_loss: 0.016184639185667038
	val_positive_acc: 0.970188075230092
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.04724191129207611
	test_negative_loss: 0.02438838593661785
	test_positive_acc: 0.9797949663565813
	test_negative_acc: 0.9940825946107135
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.705173015594482 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.55
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2336223125457764
	train_negative_loss: 2.280195474624634
	train_positive_acc: 0.7090817095567222
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2231983244419098
	train_incorrect_loss: 2.051605463027954
	train_positive_loss: 0.8338964581489563
	train_negative_loss: 0.30589190125465393
	train_correct_acc: 0.9282861060085977
	train_incorrect_acc: 0.13336817914055546
	train_positive_acc: 0.9544197324548912
	train_negative_acc: 0.47703929577741766
	train_correct_nonzero: 13930
	train_incorrect_nonzero: 1107
	train_positive_nonzero: 5314
	train_negative_nonzero: 9723
val:
	val_positive_loss: 0.05439066141843796
	val_negative_loss: 0.030192963778972626
	val_positive_acc: 0.970188075230092
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.04445017874240875
	test_negative_loss: 0.0337541326880455
	test_positive_acc: 0.9843704207511146
	test_negative_acc: 0.9938083515216394
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.779509782791138 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.575
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2335517406463623
	train_negative_loss: 2.2802343368530273
	train_positive_acc: 0.7119311531744902
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.21216635406017303
	train_incorrect_loss: 1.994668960571289
	train_positive_loss: 0.828241765499115
	train_negative_loss: 0.28594115376472473
	train_correct_acc: 0.9503539648696581
	train_incorrect_acc: 0.08974998111598673
	train_positive_acc: 0.966399616528227
	train_negative_acc: 0.4533115341280857
	train_correct_nonzero: 13133
	train_incorrect_nonzero: 1058
	train_positive_nonzero: 4868
	train_negative_nonzero: 9323
val:
	val_positive_loss: 0.05987003445625305
	val_negative_loss: 0.04382678121328354
	val_positive_acc: 0.9799919967987194
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.06960154324769974
	test_negative_loss: 0.04667108505964279
	test_positive_acc: 0.979600815154609
	test_negative_acc: 0.9965639232270657
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.71277666091919 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.6
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2334885597229004
	train_negative_loss: 2.280179023742676
	train_positive_acc: 0.7146894972368937
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.19571377336978912
	train_incorrect_loss: 2.1088755130767822
	train_positive_loss: 0.883838951587677
	train_negative_loss: 0.25877854228019714
	train_correct_acc: 0.9486754304164733
	train_incorrect_acc: 0.08680007180057954
	train_positive_acc: 0.9624886411447492
	train_negative_acc: 0.4320049783678678
	train_correct_nonzero: 12102
	train_incorrect_nonzero: 897
	train_positive_nonzero: 4175
	train_negative_nonzero: 8824
val:
	val_positive_loss: 0.0446535125374794
	val_negative_loss: 0.04249316081404686
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.03995422646403313
	test_negative_loss: 0.051596514880657196
	test_positive_acc: 0.9869867175282456
	test_negative_acc: 0.9892816784725881
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.990392446517944 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.7
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233248472213745
	train_negative_loss: 2.280139207839966
	train_positive_acc: 0.7253573937643616
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.22695939242839813
	train_incorrect_loss: 1.965535044670105
	train_positive_loss: 0.9028235673904419
	train_negative_loss: 0.3613525331020355
	train_correct_acc: 0.9354359345083647
	train_incorrect_acc: 0.17849850108183538
	train_positive_acc: 0.9699550152443918
	train_negative_acc: 0.3860396075722203
	train_correct_nonzero: 10666
	train_incorrect_nonzero: 1071
	train_positive_nonzero: 4589
	train_negative_nonzero: 7148
val:
	val_positive_loss: 0.035991519689559937
	val_negative_loss: 0.027915004640817642
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.03877007216215134
	test_negative_loss: 0.03527149558067322
	test_positive_acc: 0.9867486222901503
	test_negative_acc: 0.9899736947896044
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.939502239227295 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.8
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233032703399658
	train_negative_loss: 2.2798879146575928
	train_positive_acc: 0.7337170572829933
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2384365051984787
	train_incorrect_loss: 1.85037362575531
	train_positive_loss: 0.9156778454780579
	train_negative_loss: 0.4325506389141083
	train_correct_acc: 0.9425813196263251
	train_incorrect_acc: 0.24959401029935308
	train_positive_acc: 0.9794427512760604
	train_negative_acc: 0.3696863365199454
	train_correct_nonzero: 8382
	train_incorrect_nonzero: 1026
	train_positive_nonzero: 4118
	train_negative_nonzero: 5290
val:
	val_positive_loss: 0.035182464867830276
	val_negative_loss: 0.050864674150943756
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.026546943932771683
	test_negative_loss: 0.04046391323208809
	test_positive_acc: 0.992071303994411
	test_negative_acc: 0.9884635940986708
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 15.158401489257812 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.9
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2328503131866455
	train_negative_loss: 2.279771566390991
	train_positive_acc: 0.7412040014915415
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.25020676851272583
	train_incorrect_loss: 1.289462685585022
	train_positive_loss: 0.7042206525802612
	train_negative_loss: 0.686605334582495
	train_correct_acc: 0.941388117915041
	train_incorrect_acc: 0.40441978262706213
	train_positive_acc: 0.9795203982610377
	train_negative_acc: 0.43596327594318685
	train_correct_nonzero: 5178
	train_incorrect_nonzero: 857
	train_positive_nonzero: 2775
	train_negative_nonzero: 3260
val:
	val_positive_loss: 0.057368166744709015
	val_negative_loss: 0.17192918062210083
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.06436648964881897
	test_negative_loss: 0.1862322986125946
	test_positive_acc: 0.9859552597694642
	test_negative_acc: 0.9834093198405833
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 15.302304983139038 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.925
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232835531234741
	train_negative_loss: 2.2798756678116145
	train_positive_acc: 0.7423495478302093
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2938218414783478
	train_incorrect_loss: 1.1337491273880005
	train_positive_loss: 0.6584984660148621
	train_negative_loss: 0.8484594835440991
	train_correct_acc: 0.9419120508047153
	train_incorrect_acc: 0.48904162704942716
	train_positive_acc: 0.9816405369114222
	train_negative_acc: 0.4968465060233803
	train_correct_nonzero: 4155
	train_incorrect_nonzero: 893
	train_positive_nonzero: 2236
	train_negative_nonzero: 2812
val:
	val_positive_loss: 0.01770549640059471
	val_negative_loss: 0.21679987013339996
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9705882352941176
test:
	test_positive_loss: 0.012345941737294197
	test_negative_loss: 0.25563037395477295
	test_positive_acc: 0.9988839285714286
	test_negative_acc: 0.9474676625400786
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.971943140029907 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.95
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232799768447876
	train_negative_loss: 2.2797041489527774
	train_positive_acc: 0.7442354780786044
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.33140966296195984
	train_incorrect_loss: 0.8809449076652527
	train_positive_loss: 0.5694631934165955
	train_negative_loss: 0.9261029655753371
	train_correct_acc: 0.9398433683488745
	train_incorrect_acc: 0.6085620553362681
	train_positive_acc: 0.9790115298214074
	train_negative_acc: 0.5958687803545547
	train_correct_nonzero: 3296
	train_incorrect_nonzero: 700
	train_positive_nonzero: 1838
	train_negative_nonzero: 2158
val:
	val_positive_loss: 0.0411234088242054
	val_negative_loss: 0.07210365682840347
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.042040858417749405
	test_negative_loss: 0.10492493212223053
	test_positive_acc: 0.986978738605443
	test_negative_acc: 0.974229507404957
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.85669732093811 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.975
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232758045196533
	train_negative_loss: 2.279218815286239
	train_positive_acc: 0.7458887273052552
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.3462958037853241
	train_incorrect_loss: 0.5984770655632019
	train_positive_loss: 0.4490301012992859
	train_negative_loss: 1.1046587714614968
	train_correct_acc: 0.9552338279267754
	train_incorrect_acc: 0.7618777192775796
	train_positive_acc: 0.9893191352873032
	train_negative_acc: 0.7391134033512865
	train_correct_nonzero: 1954
	train_incorrect_nonzero: 715
	train_positive_nonzero: 981
	train_negative_nonzero: 1688
val:
	val_positive_loss: 0.03561330586671829
	val_negative_loss: 0.6783905029296875
	val_positive_acc: 1.0
	val_negative_acc: 0.6516606642657063
test:
	test_positive_loss: 0.036845140159130096
	test_negative_loss: 0.7267420291900635
	test_positive_acc: 1.0
	test_negative_acc: 0.6133694070198631
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.713691711425781 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: ltrwe
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 1.0
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232710838317871
	train_negative_loss: 2.2789623569322113
	train_positive_acc: 0.7477984962230337
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.29967787861824036
	train_incorrect_loss: 0.3630787432193756
	train_positive_loss: 0.31345489621162415
	train_negative_loss: 1.7359814825531554
	train_correct_acc: 0.9714315799652626
	train_incorrect_acc: 0.9709689275983525
	train_positive_acc: 0.9949803274404132
	train_negative_acc: 0.9475462342999347
	train_correct_nonzero: 1135
	train_incorrect_nonzero: 526
	train_positive_nonzero: 640
	train_negative_nonzero: 1021
val:
	val_positive_loss: 0.22228413820266724
	val_negative_loss: 1.0897718667984009
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 0.22352689504623413
	test_negative_loss: 1.1060625314712524
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.96928858757019 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.0
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.235914945602417
	train_negative_loss: 2.280458688735962
	train_positive_acc: 0.6209961233457092
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.10597512871026993
	train_incorrect_loss: 0.43052756673458853
	train_positive_loss: 0.10082267969846725
	train_negative_loss: 0.11288386583328247
	train_correct_acc: 0.9705409799452468
	train_incorrect_acc: 1.0
	train_positive_acc: 0.9775304658307763
	train_negative_acc: 0.963688873575837
	train_correct_nonzero: 23790
	train_incorrect_nonzero: 149
	train_positive_nonzero: 10649
	train_negative_nonzero: 13290
val:
	val_positive_loss: 0.008339991793036461
	val_negative_loss: 0.01258624717593193
	val_positive_acc: 1.0
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.016120679676532745
	test_negative_loss: 0.013423439115285873
	test_positive_acc: 0.9959492020492408
	test_negative_acc: 0.997439381270903
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.117380142211914 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.1
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2353296279907227
	train_negative_loss: 2.2804696559906006
	train_positive_acc: 0.6454344664361568
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1283857673406601
	train_incorrect_loss: 1.0295533730445996
	train_positive_loss: 0.14575384557247162
	train_negative_loss: 0.1448184996843338
	train_correct_acc: 0.9714576153066322
	train_incorrect_acc: 0.7223585231193926
	train_positive_acc: 0.9769371294953564
	train_negative_acc: 0.9539136645932702
	train_correct_nonzero: 24153
	train_incorrect_nonzero: 602
	train_positive_nonzero: 7511
	train_negative_nonzero: 17244
val:
	val_positive_loss: 0.010585015639662743
	val_negative_loss: 0.02705763280391693
	val_positive_acc: 1.0
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.02794875204563141
	test_negative_loss: 0.035549890249967575
	test_positive_acc: 0.9895238867224174
	test_negative_acc: 0.988331320690835
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.237563133239746 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.2
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.234855890274048
	train_negative_loss: 2.280496835708618
	train_positive_acc: 0.6651807662325291
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1512475460767746
	train_incorrect_loss: 0.9376084360673472
	train_positive_loss: 0.18815113604068756
	train_negative_loss: 0.17382049560546875
	train_correct_acc: 0.9705131474007811
	train_incorrect_acc: 0.7441869920203253
	train_positive_acc: 0.9731273774557342
	train_negative_acc: 0.9437501760204597
	train_correct_nonzero: 24038
	train_incorrect_nonzero: 1021
	train_positive_nonzero: 7569
	train_negative_nonzero: 17490
val:
	val_positive_loss: 0.0042775943875312805
	val_negative_loss: 0.10845011472702026
	val_positive_acc: 1.0
	val_negative_acc: 0.9803921568627452
test:
	test_positive_loss: 0.0061560822650790215
	test_negative_loss: 0.12447671592235565
	test_positive_acc: 0.9988839285714286
	test_negative_acc: 0.9701935608337027
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.356968402862549 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.3
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2344112396240234
	train_negative_loss: 2.2804107666015625
	train_positive_acc: 0.6799399308132738
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.15500035881996155
	train_incorrect_loss: 0.9202406306374278
	train_positive_loss: 0.19028788805007935
	train_negative_loss: 0.1853589564561844
	train_correct_acc: 0.9682671569745785
	train_incorrect_acc: 0.7417610177611536
	train_positive_acc: 0.977746160348472
	train_negative_acc: 0.9294229195307265
	train_correct_nonzero: 23853
	train_incorrect_nonzero: 1121
	train_positive_nonzero: 7014
	train_negative_nonzero: 17960
val:
	val_positive_loss: 0.008547463454306126
	val_negative_loss: 0.0876501202583313
	val_positive_acc: 1.0
	val_negative_acc: 0.9803921568627452
test:
	test_positive_loss: 0.013418111950159073
	test_negative_loss: 0.09455747902393341
	test_positive_acc: 0.9968319774718398
	test_negative_acc: 0.9833311627932988
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.3197762966156 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.4
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2340176105499268
	train_negative_loss: 2.280474901199341
	train_positive_acc: 0.6935097147105398
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1672971546649933
	train_incorrect_loss: 0.7838790411967159
	train_positive_loss: 0.20672442018985748
	train_negative_loss: 0.19989170134067535
	train_correct_acc: 0.971501294591756
	train_incorrect_acc: 0.8019098313871795
	train_positive_acc: 0.9792088174545398
	train_negative_acc: 0.9240855855994855
	train_correct_nonzero: 23171
	train_incorrect_nonzero: 1466
	train_positive_nonzero: 6256
	train_negative_nonzero: 18381
val:
	val_positive_loss: 0.04429585859179497
	val_negative_loss: 0.06798097491264343
	val_positive_acc: 0.9803921568627452
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.048755474388599396
	test_negative_loss: 0.07870668172836304
	test_positive_acc: 0.9835273006645734
	test_negative_acc: 0.992504769368594
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.264902591705322 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.425
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2339489459991455
	train_negative_loss: 2.280456304550171
	train_positive_acc: 0.6969199719176069
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.17537784576416016
	train_incorrect_loss: 0.75840064598424
	train_positive_loss: 0.2116287350654602
	train_negative_loss: 0.21690143644809723
	train_correct_acc: 0.9635822246404703
	train_incorrect_acc: 0.7909578091549854
	train_positive_acc: 0.9773909386469181
	train_negative_acc: 0.909914402823292
	train_correct_nonzero: 23584
	train_incorrect_nonzero: 1577
	train_positive_nonzero: 6983
	train_negative_nonzero: 18178
val:
	val_positive_loss: 0.07621634751558304
	val_negative_loss: 0.060528382658958435
	val_positive_acc: 0.959983993597439
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.06358079612255096
	test_negative_loss: 0.06187738850712776
	test_positive_acc: 0.9788311385177533
	test_negative_acc: 0.9988207547169812
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.612075090408325 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.45
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2338571548461914
	train_negative_loss: 2.2804274559020996
	train_positive_acc: 0.7004400728400598
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1857401728630066
	train_incorrect_loss: 0.7952685406209955
	train_positive_loss: 0.2257174253463745
	train_negative_loss: 0.2315257489681244
	train_correct_acc: 0.9521759126186583
	train_incorrect_acc: 0.7833101829101423
	train_positive_acc: 0.9677266615097997
	train_negative_acc: 0.8992574228120411
	train_correct_nonzero: 23563
	train_incorrect_nonzero: 1573
	train_positive_nonzero: 7150
	train_negative_nonzero: 17986
val:
	val_positive_loss: 0.014725351706147194
	val_negative_loss: 0.10761025547981262
	val_positive_acc: 1.0
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.024639734998345375
	test_negative_loss: 0.11770974099636078
	test_positive_acc: 0.992071303994411
	test_negative_acc: 0.9837070649584954
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.050650119781494 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.475
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2338180541992188
	train_negative_loss: 2.2803518772125244
	train_positive_acc: 0.7030151813933514
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.18353387713432312
	train_incorrect_loss: 0.7968657933428126
	train_positive_loss: 0.21537597477436066
	train_negative_loss: 0.23273871839046478
	train_correct_acc: 0.957598849474887
	train_incorrect_acc: 0.7732904068143103
	train_positive_acc: 0.9753148851521545
	train_negative_acc: 0.9036080603567429
	train_correct_nonzero: 23696
	train_incorrect_nonzero: 1527
	train_positive_nonzero: 7054
	train_negative_nonzero: 18169
val:
	val_positive_loss: 0.03546275198459625
	val_negative_loss: 0.025650648400187492
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.03569823130965233
	test_negative_loss: 0.030002478510141373
	test_positive_acc: 0.9886477737636052
	test_negative_acc: 0.9936274023327722
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.239074230194092 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.5
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2337594032287598
	train_negative_loss: 2.2803311347961426
	train_positive_acc: 0.7052703089654727
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.19254058599472046
	train_incorrect_loss: 0.7172278695202078
	train_positive_loss: 0.2258118838071823
	train_negative_loss: 0.24507877230644226
	train_correct_acc: 0.9589206287014399
	train_incorrect_acc: 0.802918188314792
	train_positive_acc: 0.9760177071055338
	train_negative_acc: 0.9005508028870431
	train_correct_nonzero: 23423
	train_incorrect_nonzero: 1717
	train_positive_nonzero: 6991
	train_negative_nonzero: 18149
val:
	val_positive_loss: 0.010771432891488075
	val_negative_loss: 0.12434695661067963
	val_positive_acc: 1.0
	val_negative_acc: 0.959983993597439
test:
	test_positive_loss: 0.008466530591249466
	test_negative_loss: 0.1560196876525879
	test_positive_acc: 0.9988839285714286
	test_negative_acc: 0.9619310923301061
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.167337894439697 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.525
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2336783409118652
	train_negative_loss: 2.2803027629852295
	train_positive_acc: 0.7079659133257243
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2101868838071823
	train_incorrect_loss: 0.7547985646479134
	train_positive_loss: 0.2477523386478424
	train_negative_loss: 0.2558019161224365
	train_correct_acc: 0.94980609267922
	train_incorrect_acc: 0.7895879924180992
	train_positive_acc: 0.9645950566282128
	train_negative_acc: 0.8964664278719658
	train_correct_nonzero: 23367
	train_incorrect_nonzero: 1757
	train_positive_nonzero: 7164
	train_negative_nonzero: 17960
val:
	val_positive_loss: 0.020701738074421883
	val_negative_loss: 0.05210036039352417
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.019761405885219574
	test_negative_loss: 0.06198672205209732
	test_positive_acc: 0.9949695548712205
	test_negative_acc: 0.9845000520435834
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.564436197280884 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.55
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233566999435425
	train_negative_loss: 2.280259132385254
	train_positive_acc: 0.7108896189243733
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1905357986688614
	train_incorrect_loss: 0.7243834566504626
	train_positive_loss: 0.2235455960035324
	train_negative_loss: 0.23960313200950623
	train_correct_acc: 0.9536740425272106
	train_incorrect_acc: 0.8083934955660588
	train_positive_acc: 0.9685897990233531
	train_negative_acc: 0.905943003506836
	train_correct_nonzero: 23584
	train_incorrect_nonzero: 1669
	train_positive_nonzero: 7018
	train_negative_nonzero: 18235
val:
	val_positive_loss: 0.027156490832567215
	val_negative_loss: 0.05137934535741806
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.026537586003541946
	test_negative_loss: 0.062195297330617905
	test_positive_acc: 0.9931361188092258
	test_negative_acc: 0.989533243767758
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.981486797332764 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.575
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2334976196289062
	train_negative_loss: 2.2803008556365967
	train_positive_acc: 0.7136387906013766
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2079635113477707
	train_incorrect_loss: 0.7226516384833369
	train_positive_loss: 0.244895800948143
	train_negative_loss: 0.251714825630188
	train_correct_acc: 0.9601341398867111
	train_incorrect_acc: 0.8126755897211855
	train_positive_acc: 0.9679740638947331
	train_negative_acc: 0.9030110374576239
	train_correct_nonzero: 23541
	train_incorrect_nonzero: 1920
	train_positive_nonzero: 7104
	train_negative_nonzero: 18357
val:
	val_positive_loss: 0.01825219951570034
	val_negative_loss: 0.07271473109722137
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.019195616245269775
	test_negative_loss: 0.08331692218780518
	test_positive_acc: 0.9932287114018185
	test_negative_acc: 0.9832981289666602
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.425081491470337 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.6
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2334346771240234
	train_negative_loss: 2.2802603244781494
	train_positive_acc: 0.7164321699350077
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.20664900541305542
	train_incorrect_loss: 0.7519444738752743
	train_positive_loss: 0.23312848806381226
	train_negative_loss: 0.2653581202030182
	train_correct_acc: 0.9488855955394085
	train_incorrect_acc: 0.7896463222998285
	train_positive_acc: 0.9680687528258294
	train_negative_acc: 0.8942617980999544
	train_correct_nonzero: 23618
	train_incorrect_nonzero: 1706
	train_positive_nonzero: 7402
	train_negative_nonzero: 17922
val:
	val_positive_loss: 0.020883498713374138
	val_negative_loss: 0.023121997714042664
	val_positive_acc: 1.0
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.026569917798042297
	test_negative_loss: 0.02907770685851574
	test_positive_acc: 0.9908822201737483
	test_negative_acc: 0.9923533643298343
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.742309808731079 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.7
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.233193874359131
	train_negative_loss: 2.2802343368530273
	train_positive_acc: 0.7270508954255328
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.1875283122062683
	train_incorrect_loss: 0.7096389163887666
	train_positive_loss: 0.22031468152999878
	train_negative_loss: 0.24550573527812958
	train_correct_acc: 0.9701371243692711
	train_incorrect_acc: 0.8159733943378337
	train_positive_acc: 0.9833409492075756
	train_negative_acc: 0.9069396314700378
	train_correct_nonzero: 22526
	train_incorrect_nonzero: 1899
	train_positive_nonzero: 5972
	train_negative_nonzero: 18453
val:
	val_positive_loss: 0.03367161005735397
	val_negative_loss: 0.039026081562042236
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.03759828209877014
	test_negative_loss: 0.047139961272478104
	test_positive_acc: 0.9882976337075828
	test_negative_acc: 0.996165343267965
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.182192087173462 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.8
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2329819202423096
	train_negative_loss: 2.279972791671753
	train_positive_acc: 0.7354281054865947
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.20748372375965118
	train_incorrect_loss: 0.652701155331289
	train_positive_loss: 0.23529869318008423
	train_negative_loss: 0.2864149510860443
	train_correct_acc: 0.9663866537653388
	train_incorrect_acc: 0.852969417406527
	train_positive_acc: 0.980158223892959
	train_negative_acc: 0.90895810289607
	train_correct_nonzero: 21462
	train_incorrect_nonzero: 1931
	train_positive_nonzero: 5519
	train_negative_nonzero: 17874
val:
	val_positive_loss: 0.053551048040390015
	val_negative_loss: 0.040575940161943436
	val_positive_acc: 0.9799919967987194
	val_negative_acc: 1.0
test:
	test_positive_loss: 0.048072103410959244
	test_negative_loss: 0.049517013132572174
	test_positive_acc: 0.9852635397508511
	test_negative_acc: 0.9961880210618692
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.193706512451172 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.9
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232800006866455
	train_negative_loss: 2.2799600786165866
	train_positive_acc: 0.7429167677132696
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2593313753604889
	train_incorrect_loss: 0.5472920827489356
	train_positive_loss: 0.264590859413147
	train_negative_loss: 0.6027289076024354
	train_correct_acc: 0.9513107997634904
	train_incorrect_acc: 0.8814047409284892
	train_positive_acc: 0.9775161170513188
	train_negative_acc: 0.9037780248952585
	train_correct_nonzero: 17080
	train_incorrect_nonzero: 1891
	train_positive_nonzero: 5162
	train_negative_nonzero: 13809
val:
	val_positive_loss: 0.04505259543657303
	val_negative_loss: 0.043642427772283554
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.9901960784313726
test:
	test_positive_loss: 0.03526422008872032
	test_negative_loss: 0.06980100274085999
	test_positive_acc: 0.9889752819036134
	test_negative_acc: 0.9831350767515092
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.259803056716919 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.925
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2327849864959717
	train_negative_loss: 2.2800650228427934
	train_positive_acc: 0.7440769983799328
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2350727915763855
	train_incorrect_loss: 0.5497768736031898
	train_positive_loss: 0.25174739956855774
	train_negative_loss: 0.539644573909003
	train_correct_acc: 0.9659871418382465
	train_incorrect_acc: 0.9011161035158426
	train_positive_acc: 0.9842000519609185
	train_negative_acc: 0.9232141859972914
	train_correct_nonzero: 17410
	train_incorrect_nonzero: 1759
	train_positive_nonzero: 4753
	train_negative_nonzero: 14416
val:
	val_positive_loss: 0.0011031711474061012
	val_negative_loss: 0.15620139241218567
	val_positive_acc: 1.0
	val_negative_acc: 0.9505802320928372
test:
	test_positive_loss: 0.0022084019146859646
	test_negative_loss: 0.18906955420970917
	test_positive_acc: 1.0
	test_negative_acc: 0.9508555355340967
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.25961971282959 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.95
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232747793197632
	train_negative_loss: 2.2796070633241383
	train_positive_acc: 0.7459796253585544
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.312139093875885
	train_incorrect_loss: 0.4381099033061808
	train_positive_loss: 0.30889132618904114
	train_negative_loss: 1.168735548331366
	train_correct_acc: 0.9453525113279376
	train_incorrect_acc: 0.9413262645256621
	train_positive_acc: 0.9785653330057744
	train_negative_acc: 0.9186885324119181
	train_correct_nonzero: 7816
	train_incorrect_nonzero: 1408
	train_positive_nonzero: 2568
	train_negative_nonzero: 6656
val:
	val_positive_loss: 0.04022657871246338
	val_negative_loss: 0.09967785328626633
	val_positive_acc: 0.9901960784313726
	val_negative_acc: 0.970188075230092
test:
	test_positive_loss: 0.038888536393642426
	test_negative_loss: 0.14484471082687378
	test_positive_acc: 0.9849812333475272
	test_negative_acc: 0.9643945649073611
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.321545600891113 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 0.975
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.232705593109131
	train_negative_loss: 2.279190587030875
	train_positive_acc: 0.7476839097041093
	train_negative_acc: 0.0
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.2666575312614441
	train_incorrect_loss: 0.31834083795547485
	train_positive_loss: 0.2715282142162323
	train_negative_loss: 1.8161107132729797
	train_correct_acc: 0.9696608041724494
	train_incorrect_acc: 0.984889521153994
	train_positive_acc: 0.9926086666665597
	train_negative_acc: 0.9616357746202974
	train_correct_nonzero: 1359
	train_incorrect_nonzero: 770
	train_positive_nonzero: 712
	train_negative_nonzero: 1417
val:
	val_positive_loss: 0.2864997982978821
	val_negative_loss: 0.9060219526290894
	val_positive_acc: 1.0
	val_negative_acc: 0.05062024809923969
test:
	test_positive_loss: 0.28825974464416504
	test_negative_loss: 0.9018884897232056
	test_positive_acc: 1.0
	test_negative_acc: 0.040957610370914194
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.14918065071106 seconds.
Running false_positive trial
	Config file: ./config/false_positive_experiment.json
	Results directory: ./results/false_positive_experiment
Beginning false positive dataset experiment.
	Method: sss
	Dataset: MNIST
	Samples per class: 4000
	Positive class: 4
	Negative class: 9
	Proportion of negative class with false positive label: 1.0
	Random seed: 0
	Training dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Eval dataloader kwargs: {'batch_size': 100, 'shuffle': False}
	Clean dataloader kwargs: {'batch_size': 100, 'shuffle': True}
	Clean samples per class: 100
	Validation samples per class: 100
	Model constructor: LeNet5
	Model constructor kwargs: {'overparameterize': False}
	Loss function constructor: CrossEntropyLoss
	Loss function constructor kwargs: {}
	Optimizer constructor: Adam
	Optimizer constructor kwargs: {}
	Number of epochs: 1
	Pretraining epochs: 0
	Fine-tuning epochs: 0
	Device: cuda
	Conduct initial measurements: True

Setting random seed.
Initializing and partitioning datasets.
Initializing dataloaders.
Initializing model.
LeNet5(
  (model): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Flatten(start_dim=1, end_dim=-1)
    (9): Linear(in_features=400, out_features=120, bias=True)
    (10): ReLU()
    (11): Linear(in_features=120, out_features=84, bias=True)
    (12): ReLU()
    (13): Linear(in_features=84, out_features=10, bias=True)
  )
)

Initializing loss function.
CrossEntropyLoss()

Initializing optimizer.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Measuring initial performance.
Epoch 0 complete.
train:
	train_positive_loss: 2.2326574325561523
	train_negative_loss: nan
	train_positive_acc: 0.7496250000000001
	train_negative_acc: nan
val:
	val_positive_loss: 2.2372148036956787
	val_negative_loss: 2.2795426845550537
	val_positive_acc: 0.6096438575430172
	val_negative_acc: 0.0
test:
	test_positive_loss: 2.2353570461273193
	test_negative_loss: 2.2791152000427246
	test_positive_acc: 0.6071115522223866
	test_negative_acc: 0.0
Training model.
Beginning epoch 1.
Epoch 1 complete.
train:
	train_correct_loss: 0.019714899361133575
	train_incorrect_loss: 0.018681347370147705
	train_positive_loss: 0.019198790192604065
	train_negative_loss: nan
	train_correct_acc: 0.9983333333333333
	train_incorrect_acc: 0.9988775510204081
	train_positive_acc: 0.9983333333333333
	train_negative_acc: 0.9988775510204081
	train_correct_nonzero: 206
	train_incorrect_nonzero: 194
	train_positive_nonzero: 206
	train_negative_nonzero: 194
val:
	val_positive_loss: 5.2415005484363064e-05
	val_negative_loss: 14.023241996765137
	val_positive_acc: 1.0
	val_negative_acc: 0.0
test:
	test_positive_loss: 4.41663469246123e-05
	test_negative_loss: 13.812232971191406
	test_positive_acc: 1.0
	test_negative_acc: 0.0
	Done training. Final accuracy: -inf
Trial complete.
	Time taken: 14.092082500457764 seconds.
